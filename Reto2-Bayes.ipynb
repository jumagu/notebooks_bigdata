{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a72491",
   "metadata": {},
   "source": [
    "# Reto 2 - Clasificador Bayesiano Ingenuo\n",
    "**Nombre:** Juan Manuel Gutiérrez Gómez **Código:** 2260563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414cabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25aac212",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_url = \"Rest_Mex_2022_Sentiment_Analysis_Track_Train.csv\"\n",
    "tourist_opinions = pd.read_csv(data_source_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8fc68be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Attraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pésimo lugar</td>\n",
       "      <td>Piensen dos veces antes de ir a este hotel, te...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie</td>\n",
       "      <td>Cuatro de nosotros fuimos recientemente a Eddi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relación calidad-precio</td>\n",
       "      <td>seguiré corta y simple: limpieza\\n- bad. Tengo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minusválido? ¡No te alojes aquí!</td>\n",
       "      <td>Al reservar un hotel con multipropiedad Mayan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo</td>\n",
       "      <td>No pierdan su tiempo ni dinero, venimos porque...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title  \\\n",
       "0                           Pésimo lugar   \n",
       "1              No vayas a lugar de Eddie   \n",
       "2           Mala relación calidad-precio   \n",
       "3       Minusválido? ¡No te alojes aquí!   \n",
       "4  Es una porqueria no pierdan su tiempo   \n",
       "\n",
       "                                             Opinion  Polarity  Attraction  \n",
       "0  Piensen dos veces antes de ir a este hotel, te...         1       Hotel  \n",
       "1  Cuatro de nosotros fuimos recientemente a Eddi...         1  Restaurant  \n",
       "2  seguiré corta y simple: limpieza\\n- bad. Tengo...         1       Hotel  \n",
       "3  Al reservar un hotel con multipropiedad Mayan ...         1       Hotel  \n",
       "4  No pierdan su tiempo ni dinero, venimos porque...         1       Hotel  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tourist_opinions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87d6864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\juanma\\anaconda3\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from wordcloud) (1.23.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\juanma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     11\u001b[0m wc \u001b[38;5;241m=\u001b[39m WordCloud(height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, background_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m                repeat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, mask \u001b[38;5;241m=\u001b[39m mask,\n\u001b[0;32m     13\u001b[0m                contour_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, contour_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mwc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wc, interpolation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:620\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(words)\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:582\u001b[0m, in \u001b[0;36mWordCloud.process_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    579\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_word_length \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]+\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m regexp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregexp \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregexp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pattern\n\u001b[1;32m--> 582\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# remove 's\u001b[39;00m\n\u001b[0;32m    584\u001b[0m words \u001b[38;5;241m=\u001b[39m [word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[0;32m    585\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\re.py:241\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = tourist_opinions.Opinion\n",
    "\n",
    "# Circle mask\n",
    "x, y = np.ogrid[:300, :300]\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(height = 500, width = 500, background_color = \"white\",\n",
    "               repeat = True, mask = mask,\n",
    "               contour_width = 3, contour_color = \"black\")\n",
    "wc.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation = \"bilinear\")\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf1ae5",
   "metadata": {},
   "source": [
    "## Objetivo 1: Opinión vs Atracción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1061866",
   "metadata": {},
   "source": [
    "**Conjunto de Características**\n",
    "\n",
    "Extraemos las características que analizaremos con el siguiente script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9434d54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hotel', 'Restaurant', 'Hotel', ..., 'Attractive', 'Attractive',\n",
       "       'Attractive'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tourist_opinions.iloc[:, 1].values\n",
    "labels = tourist_opinions.iloc[:, 3].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc21cd",
   "metadata": {},
   "source": [
    "Una vez que dividimos los datos en características y conjunto de entrenamiento, podemos preprocesarlos para limpiarlos. Para ello, utilizaremos expresiones regulares. Para obtener más información sobre las expresiones regulares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6af629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c043e",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Aplicamos el algoritmo de TF-IDF. La idea detrás del enfoque TF-IDF es que las palabras que aparecen menos en todos los documentos y más en un documento individual contribuyen más a la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724987fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JuanMa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('spanish'))\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33104199",
   "metadata": {},
   "source": [
    "En aras de probar la salida de nuestro clasificador, dividiremos los datos en un conjunto de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15547db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b2e463",
   "metadata": {},
   "source": [
    "### Bayesiano Ingenuo Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121dc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c464c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6831603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1005   34   16]\n",
      " [  80 2840  328]\n",
      " [ 143   87 1510]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Attractive       0.82      0.95      0.88      1055\n",
      "       Hotel       0.96      0.87      0.91      3248\n",
      "  Restaurant       0.81      0.87      0.84      1740\n",
      "\n",
      "    accuracy                           0.89      6043\n",
      "   macro avg       0.86      0.90      0.88      6043\n",
      "weighted avg       0.89      0.89      0.89      6043\n",
      "\n",
      "0.8861492636107894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4907f",
   "metadata": {},
   "source": [
    "### Bayesiano Ingenuo Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6c4c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80973128",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d548469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 999   47    9]\n",
      " [   2 3223   23]\n",
      " [   3  248 1489]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Attractive       1.00      0.95      0.97      1055\n",
      "       Hotel       0.92      0.99      0.95      3248\n",
      "  Restaurant       0.98      0.86      0.91      1740\n",
      "\n",
      "    accuracy                           0.95      6043\n",
      "   macro avg       0.96      0.93      0.95      6043\n",
      "weighted avg       0.95      0.95      0.94      6043\n",
      "\n",
      "0.945060400463346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34538e",
   "metadata": {},
   "source": [
    "## Objetivo 2: Opinión vs Sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550fa54",
   "metadata": {},
   "source": [
    "**Conjunto de Características**\n",
    "\n",
    "Extraemos las características que analizaremos con el siguiente script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ac95fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tourist_opinions.iloc[:, 1].values\n",
    "labels = tourist_opinions.iloc[:, 2].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054c1ff",
   "metadata": {},
   "source": [
    "Una vez que dividimos los datos en características y conjunto de entrenamiento, podemos preprocesarlos para limpiarlos. Para ello, utilizaremos expresiones regulares. Para obtener más información sobre las expresiones regulares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acd14bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca907c",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Aplicamos el algoritmo de TF-IDF. La idea detrás del enfoque TF-IDF es que las palabras que aparecen menos en todos los documentos y más en un documento individual contribuyen más a la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8437e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JuanMa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('spanish'))\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c46f69e",
   "metadata": {},
   "source": [
    "En aras de probar la salida de nuestro clasificador, dividiremos los datos en un conjunto de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a479ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7f60b",
   "metadata": {},
   "source": [
    "### Bayesiano Ingenuo Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e46609d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae0de0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10c59dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  55   24   17    4    4]\n",
      " [  62   46   23   12    2]\n",
      " [ 156  118   87   48   13]\n",
      " [ 280  272  229  231  151]\n",
      " [ 707  637  359  582 1924]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.04      0.53      0.08       104\n",
      "           2       0.04      0.32      0.07       145\n",
      "           3       0.12      0.21      0.15       422\n",
      "           4       0.26      0.20      0.23      1163\n",
      "           5       0.92      0.46      0.61      4209\n",
      "\n",
      "    accuracy                           0.39      6043\n",
      "   macro avg       0.28      0.34      0.23      6043\n",
      "weighted avg       0.70      0.39      0.48      6043\n",
      "\n",
      "0.38772133046500085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129afc1",
   "metadata": {},
   "source": [
    "### Bayesiano Ingenuo Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3973f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89cf10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2e69ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    2   18   30   52]\n",
      " [   0    0   19   49   77]\n",
      " [   1    0   33  151  237]\n",
      " [   0    0   10  296  857]\n",
      " [   0    0    8  265 3936]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.02      0.04       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.38      0.08      0.13       422\n",
      "           4       0.37      0.25      0.30      1163\n",
      "           5       0.76      0.94      0.84      4209\n",
      "\n",
      "    accuracy                           0.71      6043\n",
      "   macro avg       0.44      0.26      0.26      6043\n",
      "weighted avg       0.64      0.71      0.65      6043\n",
      "\n",
      "0.7061062386232004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd40741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
